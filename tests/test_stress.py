"""Stress tests for code_assistant_manager."""

import json
import os
import tempfile
import threading
import time
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from code_assistant_manager.config import ConfigManager
from code_assistant_manager.endpoints import EndpointManager
from code_assistant_manager.tools import ClaudeTool


@pytest.fixture
def stress_config():
    """Create a config file for stress testing."""
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        config_data = {
            "common": {"cache_ttl_seconds": 3600},
            "endpoints": {
                "stress-endpoint-1": {
                    "endpoint": "https://api1.stress.com/v1",
                    "api_key": "stress-key-1",
                    "description": "Stress Endpoint 1",
                    "list_models_cmd": "echo model1 model2 model3",
                    "supported_client": "claude",
                },
                "stress-endpoint-2": {
                    "endpoint": "https://api2.stress.com/v1",
                    "api_key": "stress-key-2",
                    "description": "Stress Endpoint 2",
                    "list_models_cmd": "echo model4 model5 model6",
                    "supported_client": "claude",
                },
            },
        }
        json.dump(config_data, f, indent=2)
        config_path = f.name
    yield config_path
    Path(config_path).unlink()


class TestConcurrencyStress:
    """Concurrency stress tests."""

    def test_concurrent_config_access(self, stress_config):
        """Test concurrent configuration access."""
        config = ConfigManager(stress_config)

        def worker(results, worker_id):
            try:
                for i in range(100):
                    endpoint_name = f"stress-endpoint-{(worker_id + i) % 2 + 1}"
                    value = config.get_value(endpoint_name, "endpoint")
                    results.append((worker_id, i, value))
                results.append(("success", worker_id))
            except Exception as e:
                results.append(("error", worker_id, str(e)))

        # Create multiple threads
        threads = []
        results = []

        for i in range(10):
            thread = threading.Thread(target=worker, args=(results, i))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Check results
        errors = [r for r in results if r[0] == "error"]
        successes = [r for r in results if r[0] == "success"]

        assert len(errors) == 0, f"Errors occurred: {errors}"
        assert len(successes) == 10, f"Expected 10 successes, got {len(successes)}"

    def test_concurrent_endpoint_manager_usage(self, stress_config):
        """Test concurrent endpoint manager usage."""
        config = ConfigManager(stress_config)

        def worker(results, worker_id):
            try:
                endpoint_manager = EndpointManager(config)
                for i in range(50):
                    endpoint_name = f"stress-endpoint-{(worker_id + i) % 2 + 1}"
                    success, endpoint_config = endpoint_manager.get_endpoint_config(
                        endpoint_name
                    )
                    results.append((worker_id, i, success))
                results.append(("success", worker_id))
            except Exception as e:
                results.append(("error", worker_id, str(e)))

        # Create multiple threads
        threads = []
        results = []

        for i in range(20):
            thread = threading.Thread(target=worker, args=(results, i))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Check results
        errors = [r for r in results if r[0] == "error"]
        successes = [r for r in results if r[0] == "success"]

        assert len(errors) == 0, f"Errors occurred: {errors}"
        assert len(successes) == 20, f"Expected 20 successes, got {len(successes)}"

    @patch("code_assistant_manager.endpoints.subprocess.run")
    def test_concurrent_model_fetching(self, mock_subprocess, stress_config):
        """Test concurrent model fetching."""
        mock_subprocess.return_value = MagicMock(
            stdout="model1\nmodel2\nmodel3", returncode=0
        )

        config = ConfigManager(stress_config)
        endpoint_manager = EndpointManager(config)

        endpoint_configs = {
            "stress-endpoint-1": {
                "endpoint": "https://api1.stress.com/v1",
                "actual_api_key": "stress-key-1",
                "list_models_cmd": "echo model1 model2 model3",
            },
            "stress-endpoint-2": {
                "endpoint": "https://api2.stress.com/v1",
                "actual_api_key": "stress-key-2",
                "list_models_cmd": "echo model4 model5 model6",
            },
        }

        def worker(results, worker_id):
            try:
                for i in range(50):
                    endpoint_name = f"stress-endpoint-{(worker_id + i) % 2 + 1}"
                    endpoint_config = endpoint_configs[endpoint_name]
                    success, models = endpoint_manager.fetch_models(
                        endpoint_name, endpoint_config
                    )
                    results.append(
                        (worker_id, i, success, len(models) if success else 0)
                    )
                results.append(("success", worker_id))
            except Exception as e:
                results.append(("error", worker_id, str(e)))

        # Create multiple threads
        threads = []
        results = []

        for i in range(15):
            thread = threading.Thread(target=worker, args=(results, i))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Check results
        errors = [r for r in results if r[0] == "error"]
        successes = [r for r in results if r[0] == "success"]

        assert len(errors) == 0, f"Errors occurred: {errors}"
        assert len(successes) == 15, f"Expected 15 successes, got {len(successes)}"


class TestLoadStress:
    """Load stress tests."""

    def test_high_volume_config_operations(self, stress_config):
        """Test high volume configuration operations."""
        config = ConfigManager(stress_config)

        # Perform 10,000 configuration operations
        start_time = time.time()
        for i in range(10000):
            endpoint_name = f"stress-endpoint-{i % 2 + 1}"
            config.get_value(endpoint_name, "endpoint")
            config.get_value(endpoint_name, "api_key")
        end_time = time.time()

        total_time = end_time - start_time
        ops_per_second = 20000 / total_time

        # Should handle at least 1000 operations per second
        assert (
            ops_per_second > 1000
        ), f"Only {ops_per_second:.2f} ops/second, expected > 1000"

    @patch("code_assistant_manager.endpoints.subprocess.run")
    def test_high_volume_model_fetching(self, mock_subprocess, stress_config):
        """Test high volume model fetching."""
        mock_subprocess.return_value = MagicMock(
            stdout="model1\nmodel2\nmodel3", returncode=0
        )

        config = ConfigManager(stress_config)
        endpoint_manager = EndpointManager(config)

        endpoint_config = {
            "endpoint": "https://api.stress.com/v1",
            "actual_api_key": "stress-key",
            "list_models_cmd": "echo model1 model2 model3",
        }

        # Perform 1,000 model fetch operations
        start_time = time.time()
        for i in range(1000):
            success, models = endpoint_manager.fetch_models(
                "stress-endpoint-1", endpoint_config
            )
            assert success is True
        end_time = time.time()

        total_time = end_time - start_time
        fetches_per_second = 1000 / total_time

        # Should handle at least 50 fetches per second
        assert (
            fetches_per_second > 50
        ), f"Only {fetches_per_second:.2f} fetches/second, expected > 50"

    def test_large_config_file_stress(self, tmp_path):
        """Test stress with large configuration file."""
        # Create a very large configuration file with 5000 endpoints
        large_config_file = tmp_path / "large_stress_config.json"
        endpoints = {}

        for i in range(5000):
            endpoints[f"endpoint-{i}"] = {
                "endpoint": f"https://api{i}.stress.com/v1",
                "api_key": f"stress-key-{i}",
                "description": f"Stress Endpoint {i}",
                "list_models_cmd": f"echo model{i}-1 model{i}-2",
                "supported_client": "claude",
            }

        config_data = {"common": {"cache_ttl_seconds": 3600}, "endpoints": endpoints}

        with open(large_config_file, "w") as f:
            json.dump(config_data, f, indent=2)

        # Test loading and using the large configuration
        start_time = time.time()
        config = ConfigManager(str(large_config_file))
        sections = config.get_sections()
        end_time = time.time()

        load_time = end_time - start_time

        assert len(sections) == 5000
        assert load_time < 2.0  # Should load in less than 2 seconds

    @patch("subprocess.run")
    @patch("code_assistant_manager.ui.display_centered_menu")
    @patch("code_assistant_manager.tools.select_two_models")
    @patch.dict(os.environ, {"CODE_ASSISTANT_MANAGER_NONINTERACTIVE": "1"})
    def test_high_volume_tool_executions(
        self, mock_select_models, mock_menu, mock_run, stress_config
    ):
        """Test high volume tool executions."""
        config = ConfigManager(stress_config)

        # Mock dependencies
        with patch.object(ClaudeTool, "_check_command_available", return_value=True):
            mock_menu.return_value = (True, 1)
            mock_select_models.return_value = (True, ("claude-3", "claude-2"))
            mock_run.return_value = MagicMock(returncode=0)

            with patch.object(
                EndpointManager, "get_endpoint_config"
            ) as mock_get_config:
                mock_get_config.return_value = (
                    True,
                    {
                        "endpoint": "https://api.stress.com/v1",
                        "actual_api_key": "stress-key",
                        "list_models_cmd": "echo model1 model2",
                    },
                )

                with patch.object(EndpointManager, "fetch_models") as mock_fetch_models:
                    mock_fetch_models.return_value = (True, ["claude-3", "claude-2"])

                    with patch.object(
                        EndpointManager, "select_endpoint"
                    ) as mock_select_endpoint:
                        mock_select_endpoint.return_value = (True, "stress-endpoint-1")

                        # Perform 500 tool executions
                        start_time = time.time()
                        for i in range(500):
                            tool = ClaudeTool(config)
                            result = tool.run([])
                            assert result == 0
                        end_time = time.time()

                        total_time = end_time - start_time
                        executions_per_second = 500 / total_time

                        # Should handle at least 10 executions per second
                        assert (
                            executions_per_second > 10
                        ), f"Only {executions_per_second:.2f} executions/second, expected > 10"


class TestMemoryStress:
    """Memory stress tests."""

    def test_memory_leak_config_managers(self, stress_config):
        """Test for memory leaks in ConfigManager."""
        import gc
        import sys

        # Create many ConfigManager instances and check for memory leaks
        managers = []
        initial_memory = len(gc.get_objects())

        for i in range(1000):
            manager = ConfigManager(stress_config)
            managers.append(manager)

        during_memory = len(gc.get_objects())

        # Clear references
        managers.clear()
        gc.collect()

        final_memory = len(gc.get_objects())

        # Memory should return to near initial levels
        memory_growth = final_memory - initial_memory
        assert (
            memory_growth < 100
        ), f"Memory grew by {memory_growth} objects, expected < 100"

    def test_memory_leak_endpoint_managers(self, stress_config):
        """Test for memory leaks in EndpointManager."""
        import gc

        config = ConfigManager(stress_config)

        # Create many EndpointManager instances and check for memory leaks
        managers = []
        initial_memory = len(gc.get_objects())

        for i in range(1000):
            manager = EndpointManager(config)
            managers.append(manager)

        during_memory = len(gc.get_objects())

        # Clear references
        managers.clear()
        gc.collect()

        final_memory = len(gc.get_objects())

        # Memory should return to near initial levels
        memory_growth = final_memory - initial_memory
        assert (
            memory_growth < 100
        ), f"Memory grew by {memory_growth} objects, expected < 100"

    def test_memory_leak_tools(self, stress_config):
        """Test for memory leaks in CLI tools."""
        import gc

        config = ConfigManager(stress_config)

        # Create many tool instances and check for memory leaks
        tools = []
        initial_memory = len(gc.get_objects())

        for i in range(1000):
            tool = ClaudeTool(config)
            tools.append(tool)

        during_memory = len(gc.get_objects())

        # Clear references
        tools.clear()
        gc.collect()

        final_memory = len(gc.get_objects())

        # Memory should return to near initial levels
        memory_growth = final_memory - initial_memory
        assert (
            memory_growth < 100
        ), f"Memory grew by {memory_growth} objects, expected < 100"


class TestResourceStress:
    """Resource stress tests."""

    def test_file_handle_leaks(self, stress_config):
        """Test for file handle leaks."""
        import gc
        import os

        import psutil

        process = psutil.Process(os.getpid())
        initial_fds = process.num_fds()

        # Create many ConfigManager instances that open files
        configs = []
        for i in range(1000):
            config = ConfigManager(stress_config)
            configs.append(config)

        during_fds = process.num_fds()

        # Clear references
        configs.clear()
        gc.collect()

        final_fds = process.num_fds()

        # File descriptors should return to near initial levels
        fd_growth = final_fds - initial_fds
        assert fd_growth < 10, f"File descriptors grew by {fd_growth}, expected < 10"

    @patch("code_assistant_manager.endpoints.subprocess.run")
    def test_cache_file_cleanup(self, mock_subprocess, tmp_path):
        """Test cache file cleanup under stress."""
        mock_subprocess.return_value = MagicMock(
            stdout="model1\nmodel2\nmodel3", returncode=0
        )

        # Create config with temporary directory for cache
        cache_dir = tmp_path / "cache"
        cache_dir.mkdir()

        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            config_data = {
                "common": {"cache_ttl_seconds": 3600},
                "endpoints": {
                    "cache-test": {
                        "endpoint": "https://api.cache.com/v1",
                        "api_key": "cache-key",
                        "list_models_cmd": "echo model1 model2 model3",
                    }
                },
            }
            json.dump(config_data, f, indent=2)
            config_path = f.name

        try:
            config = ConfigManager(config_path)

            # Temporarily override cache directory
            original_cache_dir = None

            # Perform many cache operations
            for i in range(100):
                endpoint_manager = EndpointManager(config)
                endpoint_config = {
                    "endpoint": "https://api.cache.com/v1",
                    "actual_api_key": "cache-key",
                    "list_models_cmd": "echo model1 model2 model3",
                }
                endpoint_manager.fetch_models("cache-test", endpoint_config)

            # Check that cache files are properly managed
            cache_files = list(cache_dir.glob("*.txt"))
            # Should not have excessive cache files
            assert len(cache_files) <= 10

        finally:
            Path(config_path).unlink()


class TestErrorRecoveryStress:
    """Error recovery stress tests."""

    @patch("code_assistant_manager.endpoints.subprocess.run")
    def test_concurrent_error_handling(self, mock_subprocess, stress_config):
        """Test concurrent error handling."""
        from subprocess import TimeoutExpired

        # Simulate alternating success and failure
        call_count = 0

        def mock_run_side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count % 3 == 0:
                # Simulate timeout every 3rd call
                raise TimeoutExpired("cmd", 30)
            elif call_count % 5 == 0:
                # Simulate command error every 5th call
                return MagicMock(stdout="", stderr="Error", returncode=1)
            else:
                # Success
                return MagicMock(stdout="model1\nmodel2\nmodel3", returncode=0)

        mock_subprocess.side_effect = mock_run_side_effect

        config = ConfigManager(stress_config)
        endpoint_manager = EndpointManager(config)

        endpoint_config = {
            "endpoint": "https://api.error.com/v1",
            "actual_api_key": "error-key",
            "list_models_cmd": "echo model1 model2 model3",
        }

        # Perform 100 fetch operations with mixed success/failure
        results = []
        for i in range(100):
            try:
                success, models = endpoint_manager.fetch_models(
                    "stress-endpoint-1", endpoint_config
                )
                results.append(
                    ("success" if success else "failure", len(models) if success else 0)
                )
            except Exception as e:
                results.append(("exception", str(e)))

        # Should handle all cases without crashing
        assert len(results) == 100
        # Most should be successful
        success_count = len([r for r in results if r[0] == "success"])
        assert (
            success_count > 50
        ), f"Only {success_count} successes out of 100 operations"
